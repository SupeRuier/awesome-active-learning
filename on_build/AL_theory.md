### Optimum performance boundary
- [Towards Understanding the Optimal Behaviors of Deep Active Learning Algorithms [2021]](https://arxiv.org/pdf/2101.00977.pdf):
  Point out that there is little study on what the optimal AL algorithm looks like, which would help researchers understand where their models fall short and iterate on the design.
  Present a simulated annealing algorithm to search for an optimal strategy and analyze it for several different tasks.
- [On Statistical Bias In Active Learning: How And When To Fix It [2021, ICLR]](https://arxiv.org/pdf/2101.11665.pdf)
- [Feedback Coding for Active Learning](https://arxiv.org/pdf/2103.00654.pdf): Formulate general active learning problems in terms of a feedback coding system, and a demonstration of this approach through the application and analysis of active learning in logistic regression.

### Adversarial Examples

- Exploring Adversarial Examples for Efficient Active Learning in Machine Learning Classifiers [2021]: Our theoretical proofs provide support to more efficient active learning methods with the help of adversarial examples, contrary to previous works where adversar- ial examples are often used as destructive solutions

### Agnostic Active Learning
- Agnostic Active Learning [2006, ICML]
- Beyond Disagreement-based Agnostic Active Learning [2014, NeurIPS]

### Adversarial Label Corruptions

- [Corruption Robust Active Learning [2021, Arxiv]](https://arxiv.org/pdf/2106.11220.pdf): 
  Under a streaming-based active learning for binary classification setting.

### Active Class Selection

- Certification of Model Robustness in Active Class Selection [2021]

### Representative-Based Approach

- Robust active representation via $l_{2,p}$-norm constraints [2021, KBS]

### Sampling Bias

- [On Statistical Bias In Active Learning: How and When to Fix It [2021, ICLR]](https://openreview.net/pdf?id=JiYq3eqTKY):
  Active learning can be helpful not only as a mechanism to reduce variance as it was originally designed, but also because it introduces a bias that can be actively helpful by regularizing the model.

### Stream-Based AL

- Neural Active Learning with Performance Guarantees [2021, NeuraIPS]:
  Non-parametric regimes.
- learning with Labeling Induced Abstentions [2021, NeuraIPS]:
  The performance should only be evaluated on the rest unlabeled instances.

### Deep AL 

- Investigating the Consistency of Uncertainty Sampling in Deep Active Learning [2021, DAGM-GCPR]